#!/bin/bash

# May 2021 - Gene Weber 
# This file sets up the variables required to install, configure, and run
# the Accumulo 2.0.1 benchmark test.

# vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
# >>> System Related Variables <<<
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

# Location of existing directory on shared file system to install software.
# Provide fully qualified path.
export INSTALL_DIR=/nfs/software/test
echo $INSTALL_DIR > inst_dir

# Use logname to determine if this is running on AWS.
if [ "$LOGNAME" == "centos" ]; then
  export AWS=true
else
  export AWS=false
fi

# Capture the hostname of the local node
if [ "$AWS" = true ]; then
  export LOCAL_NODE=$(hostname -i)
else
  export LOCAL_NODE=$(hostname)
fi

# List names of NVME drive mount points (initial "/" assumed)
export NVME_MOUNTS="\
local_nvme"
# ioDrive1 \
# ioDrive2"

# List names of NVME drives (/dev/ is assumed)
# df /local_nvme
export NVME_DRIVES="\
nvme0n1"
# nvme0n1 \
# nvme0n2"

# Set name of lowest directory on NVME drive with user read, write, and execute permission.
# Use "" if /
export LOCAL_BASE_DIR=""

# Set Name of first local NVME drive
set -- $NVME_MOUNTS
export LOCAL_NVME1=${1}${LOCAL_BASE_DIR}

# Get the total amount of worker node memory
export TOTAL_MEM=$(free -m | grep "Mem:" | sed 's/Mem: *//' | sed 's/ .*$//')

# Get the number of CPUs per node
export VCPUS=$(nproc)

# Open files limit. System must be set to allow this many or more open files.
# ulimit -n
export OF_ULIMIT=100000

# The following code generates a range of IP addresses. Rather than type in a long list of
# consecutive IP addresses for nodes, this creates a variable that is a list of nodes. Simply
# insert this varible where an IP address would go. For example any of these are valid:
#
# export SOME_NODES="\   export SOME_NODES="\   export SOME_NODES="\
# 10.3.1.110 \           10.3.1.110 \           $RANGE1"
# $RANGE1 \              $RANGE1"
# 10.3.1.300"
#
# Duplicate this code (with unique variable names) for as many ranges as are needed. 
RANGE1_BASE="10.3.1."; RANGE1_START=117; RANGE1_END=127
for (( i=$RANGE1_START; i<=$RANGE1_END; i++ ))
do
  RANGE1="$RANGE1
          ${RANGE1_BASE}${i}"
done

# List of all_nodes that will be used for Hadoop, Zookeeper, and Accumulo
export ALL_NODES="\
10.114.89.201 \
10.114.89.206 \
10.114.89.207 \
10.114.89.212 \
10.114.89.221 \
10.114.89.225 \
10.114.89.234 \
10.114.89.238 \
10.114.89.243 \
10.114.89.248 \
10.114.89.252 \
10.114.89.254"

# vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
# >>> Start of Hadoop Configuration <<<
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

# Hadoop tarball and path information.
export HADOOP="hadoop-3.1.2.tar.gz"
export HADOOP_HOME="${INSTALL_DIR}/hadoop-3.1.2"
export HADOOP_PATH="${HADOOP_HOME}/bin:$PATH"
export HADOOP_CONF_DIR="${HADOOP_HOME}/etc/hadoop"

# Set up Hadoop resouce node, name node, and worker nodes.
export HADOOP_NAME_NODE="10.114.89.201"
export HADOOP_RESOURCE_NODE="10.114.89.206"
export HADOOP_WORKER_NODES="\
10.114.89.207 \
10.114.89.212 \
10.114.89.221 \
10.114.89.225 \
10.114.89.234 \
10.114.89.238 \
10.114.89.243 \
10.114.89.248 \
10.114.89.252 \
10.114.89.254"
export NUM_WORKER_NODES=10

# Calculate  or set various memory allocation values in MB for use in config files.
#
# Hadoop JobHistory server - HADOOP_JOB_HISTORYSERVER_HEAPSIZE
export HAD_HIS_HEAP=5000

# Hadoop Name Node - HADOOP_NAMENODE_HEAPSIZE
export HAD_NAM_HEAP=15000

# Hadoop Data Node - HADOOP_DATANODE_HEAPSIZE
export HAD_DAT_HEAP=15000

# MapReduce Application Master - yarn.app.mapreduce.am.resource.mb
export YARN_MR_RES=10000

# Minimum allocation for every container - yarn.scheduler.minimum-allocation-mb
export YARN_SCH_MIN=1024

# Maximum allocation for every container - yarn.scheduler.maximum-allocation-mb
export YARN_SCH_MAX=$(expr $(expr $TOTAL_MEM \* 68) / 100)

# What can be allocated for containers - yarn.nodemanager.resource.memory-mb
export YARN_NM_RES=$(expr $(expr $TOTAL_MEM \* 75) / 100)

# Yarn Resource Manager Heap - YARN_RESOURCEMANAGER_HEAPSIZE
export YARN_RM_HEAP=10000

# Yarn Node Manager Heap - YARN_NODEMANAGER_HEAPSIZE
export YARN_NM_HEAP=5000

# Yarn Proxy Server Heap - YARN_PROXYSERVER_HEAPSIZE
export YARN_PRX_HEAP=3000

# Amount for each map task - mapreduce.map.java.opts
export MAPRED_MAP_JOPT=$(expr $(expr $TOTAL_MEM \* 18) / 100)

# Note: 22.5% Hadoop Map limit - mapreduce.map.memory.mb
export MAPRED_MAP_MEM=$(expr $(expr $TOTAL_MEM \* 45) / 200)

# Amount for each reduce task - mapreduce.reduce.java.opts
export MAPRED_RED_JOPT=$(expr $(expr $TOTAL_MEM \* 36) / 100)

# Hadoop Reduce limit - mapreduce.reducemap.memory.mb
export MAPRED_RED_MEM=$(expr $(expr $TOTAL_MEM \* 45) / 100)

# Note: In bytes. Space on local disk protected from Hadoop.
# - dfs.datanode.du.reserved
export RESERVED_SPACE=10000000000


# Calculate  or set various vcores alloctions based on number of CPUs/node
#
# Minimum number of vcores to allocate to yarn container.
# yarn.scheduler.minimum-allocation-vcores
if [ "$VCPUS" -le "4" ]; then
export YARN_VCORES_MIN=1
elif [ "$VCPUS" -le "8" ]; then
export YARN_VCORES_MIN=2
elif [ "$VCPUS" -le "16" ]; then
export YARN_VCORES_MIN=3
elif [ "$VCPUS" -le "32" ]; then
export YARN_VCORES_MIN=4
elif [ "$VCPUS" -le "64" ]; then
export YARN_VCORES_MIN=5
else
export YARN_VCORES_MIN=6
fi

# Maximum number of vcores to allocate to a yarn container.
# yarn.scheduler.maximum-allocation-vcores
export YARN_VCORES_MAX=$(expr $(expr $NUM_WORKER_NODES / 2) \* $VCPUS)

# Number of vcores to allocate to MR AppMaster
# yarn.app.mapreduce.am.resource.cpu-vcores
if [ "$VCPUS" -le "4" ]; then
export YARN_AM_VCORES=1
elif [ "$VCPUS" -le "16" ]; then
export YARN_AM_VCORES=2
elif [ "$VCPUS" -le "32" ]; then
export YARN_AM_VCORES=3
elif [ "$VCPUS" -le "64" ]; then
export YARN_AM_VCORES=4
else
export YARN_AM_VCORES=5
fi

# Number of vcores to allocate to a map job
# mapreduce.map.cpu.vcores
export MAPRED_MAP_VCORES=$YARN_VCORES_MAX

# Number of vcores to allocate to a reduce job
# mapreduce.reduce.cpu.vcores
export MAPRED_RED_VCORES=$(expr $MAPRED_MAP_VCORES / 2)

# Location of temporary Hadoop directorie on local NVMe drive
export TMP_HADOOP_DIRS='tmp/hadoop-${user.name}'

# Location of Hadoop PID info on local NVMe drive
export H_PID_DIR="pids"

# Location of Hadoop logs on local NVMe drive
export H_LOG_DIR="data/hadoop"

# Directory on local NVMe drive used to temporarily save out-of-order writes before writing to HDFS
export OOO_WR_DIR="tmp/.hdfs-nfs"

# vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
# >>> Start of Zookeeper Configuration <<<
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

# Zookeeper tarball and path information.
export ZOOKEEPER="apache-zookeeper-3.6.2-bin.tar.gz"
export ZOO_HOME="${INSTALL_DIR}/zookeeper-3.6.2"
export ZOO_PATH="${INSTALL_DIR}/zookeeper-3.6.2/bin/:$PATH"

# Set up Zookeeper nodes. There should be 1, 3, or 5 nodes.
export ZOOKEEPER_NODES="\
10.114.89.201 "
# 10.114.89.206 \
# 10.114.89.207 \
export NUM_ZOO_NODES=1

# vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
# >>> Start of Accumulo Configuration <<<
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

# Accumulo tarball and path information.
export ACCUMULO="accumulo-2.0.1-bin.tar.gz"
export ACCUMULO_HOME="${INSTALL_DIR}/accumulo-2.0.1"
export ACCUMULO_PATH="${ACCUMULO_HOME}/bin:$PATH"

# Path to lib64 of gcc version that the Accumulo native library was compiled with.
export LIB64=/opt/rh/devtoolset-8/root/usr/local/lib64

# Set up Accumulo master, garbage collector, monitor, tracer, and tserver nodes.
export ACC_MASTER_NODES="10.114.89.206"
export ACC_MON_NODES="10.114.89.212"
export ACC_GC_NODES="10.114.89.221"
export ACC_TRACER_NODES="10.114.89.225"
export ACC_TSERVER_NODES="\
10.114.89.234 \
10.114.89.238 \
10.114.89.243 \
10.114.89.248 \
10.114.89.252 \
10.114.89.254"
export NUM_TSERVER_NODES=6

# Number of tablet servers per tserver node
TABS_PER_NODE=3

# Accumulo database name
export ACC_DBASE_NAME=Grogu

# Zookeeper connection information
ZOO_INSTANCES=""
for inst in $ZOOKEEPER_NODES
do
  if [ "$ZOO_INSTANCES" != "" ]; then
    ZOO_INSTANCES="$ZOO_INSTANCES,"
  fi
  ZOO_INSTANCES="${ZOO_INSTANCES}${inst}:2181"
done
export ZOO_INSTANCES

# Accumulo principal username
export ACC_USERNAME="root"

# Accumulo authentication token (password)
export ACC_PASSWD="secret"

# Zookeeper session timeout
export ZOO_SESSION_TIMEOUT="300s"

# Max amount of time (in seconds) an unresponsive server will be re-tried.
export BATCH_WRITE_TIMEOUT="300"

# A comma separated list of dfs uris to use.
export DFS_INSTANCE_VOL="${HADOOP_NAME_NODE}:9000"

# Location of Accumulo logs on local NVMe Drive
export ACC_LOG_DIR="data/accumulo/logs"

# Location of ACCUMULO PID info on local NVMe drive
export A_PID_DIR="pids"

# The port client range for handling client connections on the tablet servers
export TSERV_PORT_CLIENT=11111-11199

# vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
# >>> Start of Maven Configuration <<<
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

# Maven 3.6.3 tests tarball and path information.
export MAVEN="apache-maven-3.6.3-bin.tar.gz"
export MAVEN_HOME="${INSTALL_DIR}/apache-maven-3.6.3"
export MAVEN_PATH="${MAVEN_HOME}/bin:$PATH"

# vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
# >>> Start of Accumulo Tests Configuration <<<
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

# Accumulo 2.0 tests tarball and path information.
export ACCUMULO_TESTS="2.0.zip"
export ACCUMULO_TESTS_HOME="${INSTALL_DIR}/accumulo-testing-2.0"
export ACCUMULO_TESTS_PATH="${ACCUMULO_TESTS_HOME}/bin:$PATH"

# Amount of time to wait for Accumulo to stabilize after a new tablet is created without pre-split.
export NEW_TAB_WAIT_NOSPLIT=1m

# Amount of time to wait for Accumulo to stabilize after a new tablet is created with pre-split.
export NEW_TAB_WAIT_SPLIT=3m

# Amount of time to wait for Accumulo to stop modifying rfiles after table set offline.
export OFFLINE_TAB_WAIT=5m

# vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
# >>> Start of Accumulo Proxy Configuration <<<
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

# Accumulo Proxy tarball and path information.
export ACC_PROXY_ZIP="main.zip"
export ACC_PROXY="accumulo-proxy-2.0.0-SNAPSHOT-bin.tar.gz"
export ACC_PROXY_BUILD="${INSTALL_DIR}/accumulo-proxy-main"
export ACC_PROXY_HOME="${INSTALL_DIR}/accumulo-proxy-2.0.0"
export ACC_PROXY_PATH="${ACC_PROXY_HOME}/bin:$PATH"

# Select whether or not to install the accumulo proxy.
export INSTALL_PROXY=true

# Node to run proxy on.
export ACC_PROXY_NODE="10.114.89.206"

# Add to path if installed.
if [ $INSTALL_PROXY = true ]; then
  ACC_PROXY_HOME_STR="export ACC_PROXY_HOME=$ACC_PROXY_HOME"
  ACC_PROXY_PATH="export PATH=\$ACC_PROXY_HOME/bin:\$PATH"
else
  ACC_PROXY_HOME_STR=""
  ACC_PROXY_PATH=""
fi

# vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
# >>> Start of Pi and Terasort mapreduce test variables <<<
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

# PI and Terasort - https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.1.3/bk_using-apache-hadoop/content/running_mapreduce_examples_on_yarn.html

# Hadoop pi mapreduce test setup
export PI_NUM_MAPS=16
export PI_NUM_SAMPLES=1000000

# Hadoop terasort test setup
# Rows are 100 bytes long, so total data is 100 times number of rows (100 GB of data, use 1,000,000,000 rows)
export TERASORT_ROWS=10000

# vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
# >>> Start of Ingest test variables <<<
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

# Ingest Clients per tserver. 3 is a good starting point.
export INGEST_CLI_TSERVER=3
export INGEST_CLI_NODE=$(expr $TABS_PER_NODE \* $INGEST_CLI_TSERVER)

# Run time options. See accumulo-testing-2.0/conf/accumulo-testing.properties for a full list of options.
export CI_OPTS="-o test.ci.ingest.pause.enable=false"

# Percentage in decimal (1=100%) of HDFS file system to fill with the Accumulo continuous ingest database
CI_FILL_PERCENT=0.0025

# vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
# >>> Start of Bulk Ingest test variables <<<
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

# Node to run bulk ingest on
export BI_NODE=10.114.89.206

# Number of map loops to run. Nodes are generated per map task. Map tasks are run each loop.
export BI_LOOPS=2

# When to import data. Import data each Iteration, or import all data at the End (i/e)
export BI_WHEN="i"

# Should a verification of the ingest table be run (y/n)
export BI_VERIFY="y"

# Should ingest table be reset at start (y/n)
export BI_RESET_TABLE="y"

# vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
# >>> Start of Walk test variables <<<
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

# Number of Walk Clients to run per tserver node.
export WALK_CLI_NODE=3

# The sleep time between scans in ms.
# NOTE: time seems to actually be 1/10 of this value.
export DEL_BTWN_WALKS=0

# Scan batch size for batch walks.
export SCAN_BATCH_SIZE=1000

# Approximate duration to run the test_walk test.
# SUFFIX may be 's' for seconds, 'm' for minutes, 'h' for hours or 'd' for days.
export TEST_WALK_DURATION=5m

# vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
# >>> Start of Moru table test variables <<<
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

# Node to run Moru table test on
export MT_NODE=10.114.89.201

# vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
# >>> Start of Verify table test variables <<<
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

# Node to run Verify table test on
export VER_NODE=10.114.89.206

# vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
# >>> Start Create acc_env_vars Environmental Varibles File to be Source by .bashrc <<<
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

if [ ! -f $INSTALL_DIR/acc_env_vars ]; then
cat <<EOT > $INSTALL_DIR/acc_env_vars
#!/bin/bash

# Gene Weber - April 2021
# This file must be sourced by .bashrc

INSTALL_DIR=$INSTALL_DIR

# Environmental Variables to set.
export ZOOKEEPER_HOME=$ZOO_HOME
export ZOOKEEPER_CONF_DIR=\$ZOOKEEPER_HOME/conf
export PATH=\$ZOOKEEPER_HOME/bin:\$PATH
export HADOOP_HOME=$HADOOP_HOME
export HADOOP_CONF_DIR=\$HADOOP_HOME/etc/hadoop
export PATH=\$HADOOP_HOME/bin:\$PATH
export MAPR=\$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.2.jar
export ACCUMULO_HOME=$ACCUMULO_HOME
export ACCUMULO_CONF_DIR=\$ACCUMULO_HOME/conf
export PATH=\$ACCUMULO_HOME/bin:\$PATH
export ACCUMULO_TESTS_HOME=$ACCUMULO_TESTS_HOME
export PATH=\$ACCUMULO_TESTS_HOME/bin:\$PATH
export MAVEN_HOME=$MAVEN_HOME
export PATH=\$MAVEN_HOME/bin:\$PATH
export AWS=$AWS
$ACC_PROXY_HOME_STR
$ACC_PROXY_PATH
EOT
chmod 755 $INSTALL_DIR/acc_env_vars
rm write_new_acc_env_vars 2>/dev/null
fi
